{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21caf431-641b-49ae-8669-88243d86e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5725e39-17eb-41f1-add4-4256782f946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "with open('2.txt', 'r') as f:\n",
    "    glove_vocab = [line.strip() for line in f]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4631ccf-ca80-49c0-a03a-03585b1c854c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdac0a8e-c187-497f-b5cd-fd042708e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embeddings(glove_vocab, batch_size=16):\n",
    "    word_embeddings = {}\n",
    "    \n",
    "    for i in range(0, len(glove_vocab), batch_size):\n",
    "        batch_words = glove_vocab[i:i + batch_size]\n",
    "        embeddings = helper(batch_words)\n",
    "        for word, embedding in zip(batch_words, embeddings):\n",
    "            word_embeddings[word] = embedding\n",
    "    return word_embeddings\n",
    "\n",
    "def helper(words):\n",
    "    inputs = tokenizer(words, return_tensors='pt', padding=True, truncation=True, is_split_into_words=False)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to the device\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    token_embeddings = outputs.last_hidden_state  \n",
    "    word_embeddings = token_embeddings.mean(dim=1) \n",
    "    return word_embeddings.cpu().numpy() \n",
    "\n",
    "\n",
    "\n",
    "word_embeddings = word_embeddings(glove_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4c728a5-0d5e-4ff0-af12-09cec80875bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(vectors):\n",
    "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)  # L2 norm of each vector\n",
    "    return vectors / norms  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c3e734-48c0-4d0f-9702-c59ef5cad6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_index = {word: idx for idx, word in enumerate(word_embeddings.keys())}\n",
    "index_to_key = {idx: word for word, idx in key_to_index.items()}\n",
    "\n",
    "vectors = np.array(list(word_embeddings.values()))  \n",
    "vectors_normalized = normalize_embeddings(vectors)\n",
    "\n",
    "\n",
    "def most_similar_words(word, topn=10):\n",
    "    if word not in key_to_index:\n",
    "        return f\"Word '{word}' not found in the vocabulary.\"\n",
    "    \n",
    "    word_id = key_to_index[word]\n",
    "    emb = vectors_normalized[word_id]\n",
    "    emb_normalized = emb #/ np.linalg.norm(emb)  \n",
    "    similarities = vectors_normalized @ emb_normalized\n",
    "    ids_ascending = similarities.argsort()\n",
    "    ids_descending = ids_ascending[::-1]\n",
    "    mask = ids_descending != word_id\n",
    "    ids_descending = ids_descending[mask]\n",
    "    top_ids = ids_descending[:topn]\n",
    "    top_words = [(index_to_key[i], similarities[i]) for i in top_ids]\n",
    "  \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc0f8cd-396f-43ba-bcf4-103cb865ebe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cakes', 0.99903613),\n",
       " ('knife', 0.9978445),\n",
       " ('forest', 0.9974806),\n",
       " ('humans', 0.997444),\n",
       " ('bird', 0.9974233),\n",
       " ('fiction', 0.99741656),\n",
       " ('flower', 0.9974063),\n",
       " ('species', 0.9973448),\n",
       " ('fighter', 0.99729323),\n",
       " ('fork', 0.9972407)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words(\"cake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6fffa1c-054b-49bb-b0b7-f806681b3bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cumin', 0.9924763),\n",
       " ('cabbage', 0.99220216),\n",
       " ('camel', 0.9919967),\n",
       " ('cinnamon', 0.9912886),\n",
       " ('poultry', 0.9910093),\n",
       " ('crimp', 0.9909573),\n",
       " ('cubes', 0.9906302),\n",
       " ('carrots', 0.9905422),\n",
       " ('cotton', 0.9904048),\n",
       " ('parrots', 0.9899977)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81cc8aa4-5c69-47b0-a72c-978654fe4246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('had', 0.998102),\n",
       " ('were', 0.99799585),\n",
       " ('been', 0.99789417),\n",
       " ('has', 0.99786943),\n",
       " ('his', 0.9977933),\n",
       " ('could', 0.9977186),\n",
       " ('have', 0.9976024),\n",
       " ('also', 0.9975828),\n",
       " ('they', 0.9974645),\n",
       " ('can', 0.99742717)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words(\"between\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67b82205-f623-4e9e-a12d-ad3a0e692bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hungry', 0.9923549),\n",
       " ('wry', 0.9903593),\n",
       " ('garry', 0.9884526),\n",
       " ('husbandry', 0.9875644),\n",
       " ('ferry', 0.9871393),\n",
       " ('awry', 0.98678946),\n",
       " ('derry', 0.9867022),\n",
       " ('scary', 0.98658115),\n",
       " ('merry', 0.98640233),\n",
       " ('unruly', 0.9863487)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_words(\"angry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc9b94c-9196-45ab-896f-09c7d588210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c549ec11-f3cb-476d-b5b4-8a21ce4a24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize and average token embeddings for a word\n",
    "def get_word_embedding(word):\n",
    "    # Tokenize the word into subwords\n",
    "    inputs = tokenizer(word, return_tensors='pt')\n",
    "    \n",
    "    # Get token embeddings (last hidden states from the model)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Average the token embeddings to get the word embedding\n",
    "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: (num_tokens, embedding_dim)\n",
    "    word_embedding = token_embeddings.mean(dim=0)  # Shape: (embedding_dim,)\n",
    "    \n",
    "    return word_embedding.numpy()\n",
    "\n",
    "# Create word embeddings for all words in the GloVe vocabulary\n",
    "word_embeddings = {}\n",
    "#for word in glove_vocab:\n",
    "    #print(word)\n",
    "    #word_embeddings[word] = get_word_embedding(word)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
